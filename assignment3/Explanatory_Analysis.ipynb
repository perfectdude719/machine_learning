{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load the generated dataset\n",
    "df = pd.read_csv(\"MyData_updated.csv\") \n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['smoking','hearing(left)', 'Cholesterol', 'ALT', 'eyesight(left)','hearing(right)', 'dental caries'])  # drop all except(waist, hemoglobin, weight, serum creatinine)\n",
    "y = df['smoking']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X and y to numpy arrays for clarity\n",
    "X_scaled = np.array(X_scaled)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (159256, 4)\n",
      "Validation set size: (31851, 4)\n",
      "Test set size: (31852, 4)\n"
     ]
    }
   ],
   "source": [
    "# Split into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.4, random_state=42, stratify=y)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Training set size:\", X_scaled.shape)\n",
    "print(\"Validation set size:\", X_valid.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the training set:\n",
      "[[-1.45145365e+00 -6.26718756e-01 -9.64842581e-01 -5.17239273e-01]\n",
      " [-2.22136427e-04 -9.76073844e-01 -1.36210438e+00 -1.63240593e+00]\n",
      " [ 7.81210215e-01 -9.76073844e-01  2.26942811e-01  1.15551071e+00]]\n",
      "\n",
      "First few rows of the validation set:\n",
      "[[-0.80398113  1.6091538  -0.17031899  0.59792738]\n",
      " [-0.10069201  0.63095956  0.22694281  1.15551071]\n",
      " [-1.17237066 -0.27736367 -0.96484258 -0.51723927]]\n",
      "\n",
      "First few rows of the test set:\n",
      "[[-0.22348852  1.88863787 -0.17031899  0.04034405]\n",
      " [ 0.33467744  0.56108854  0.62420461  1.15551071]\n",
      " [-0.67002129  0.77070159 -0.17031899  0.59792738]]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the processed datasets\n",
    "print(\"First few rows of the training set:\")\n",
    "print(X_train[:3])\n",
    "\n",
    "print(\"\\nFirst few rows of the validation set:\")\n",
    "print(X_valid[:3])\n",
    "\n",
    "print(\"\\nFirst few rows of the test set:\")\n",
    "print(X_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape (n_estimators, n_samples): (100, 300)\n",
      "Majority vote shape (n_samples,): (300,)\n",
      "Bagging Accuracy: 0.8566666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class BaggingClassifier:\n",
    "    def __init__(self, base_estimators=None, n_estimators=100, random_state=None):\n",
    "        \"\"\"\n",
    "        Bagging ensemble classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - base_estimators: List of base models to use for bagging (default: [DecisionTreeClassifier()]).\n",
    "        - n_estimators: Number of estimators/models in the ensemble.\n",
    "        - random_state: Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.base_estimators = base_estimators or [DecisionTreeClassifier()]\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the bagging classifier by fitting multiple base estimators on bootstrapped samples.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        self.models = []\n",
    "        n_estimators_per_model = self.n_estimators // len(self.base_estimators)\n",
    "        \n",
    "        for base_estimator in self.base_estimators:\n",
    "            for _ in range(n_estimators_per_model):\n",
    "                # Create a bootstrap sample\n",
    "                indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "                X_bootstrap = X[indices]\n",
    "                y_bootstrap = y[indices]\n",
    "                \n",
    "                # Train a new base model on the bootstrap sample\n",
    "                model = clone(base_estimator)\n",
    "                model.fit(X_bootstrap, y_bootstrap)\n",
    "                self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for the input data by majority voting.\n",
    "        \"\"\"\n",
    "        # Collect predictions from each model\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        \n",
    "        # Perform majority voting\n",
    "        majority_vote = mode(predictions, axis=0).mode.flatten()\n",
    "        return majority_vote, predictions\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data for testing\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define a list of diverse base classifiers\n",
    "base_estimators = [\n",
    "    DecisionTreeClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SVC(probability=True),  # Ensure SVC supports probability estimation\n",
    "    KNeighborsClassifier(),\n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "# Initialize and train the Bagging ensemble\n",
    "bagging_model = BaggingClassifier(base_estimators=base_estimators, n_estimators=100, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "majority_vote, predictions = bagging_model.predict(X_valid)\n",
    "print(\"Predictions shape (n_estimators, n_samples):\", predictions.shape)\n",
    "print(\"Majority vote shape (n_samples,):\", majority_vote.shape)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_valid, majority_vote)\n",
    "print(\"Bagging Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "class AdaBoostClassifier:\n",
    "    def __init__(self, base_estimators=None, n_estimators=100, random_state=None):\n",
    "        \"\"\"\n",
    "        AdaBoost ensemble classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - base_estimators: List of base models to use for boosting.\n",
    "        - n_estimators: Total number of models in the ensemble.\n",
    "        - random_state: Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.base_estimators = base_estimators or [DecisionTreeClassifier(max_depth=1, random_state=random_state)]\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.models = []\n",
    "        self.alphas = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the AdaBoost classifier using weighted training samples.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples = len(X)\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "        self.models = []\n",
    "        self.alphas = []\n",
    "\n",
    "        n_estimators_per_model = self.n_estimators // len(self.base_estimators)\n",
    "\n",
    "        for base_estimator in self.base_estimators:\n",
    "            for _ in range(n_estimators_per_model):\n",
    "                # Clone the base estimator\n",
    "                model = clone(base_estimator)\n",
    "                \n",
    "                # Check if the model supports `sample_weight`\n",
    "                if hasattr(model, \"fit\") and \"sample_weight\" in model.fit.__code__.co_varnames:\n",
    "                    model.fit(X, y, sample_weight=weights)\n",
    "                else:\n",
    "                    # Resample the dataset based on the weights manually\n",
    "                    indices = np.random.choice(n_samples, size=n_samples, replace=True, p=weights)\n",
    "                    X_resampled, y_resampled = X[indices], y[indices]\n",
    "                    model.fit(X_resampled, y_resampled)\n",
    "                \n",
    "                # Predict on the full dataset\n",
    "                y_pred = model.predict(X)\n",
    "\n",
    "                # Calculate error and alpha\n",
    "                incorrect = (y_pred != y)\n",
    "                error = np.dot(weights, incorrect) / np.sum(weights)\n",
    "\n",
    "                # Avoid division by zero or invalid alpha calculation\n",
    "                if error >= 1.0:\n",
    "                    continue\n",
    "                alpha = 0.5 * np.log((1 - error) / error) if error > 0 else 1.0\n",
    "                self.alphas.append(alpha)\n",
    "\n",
    "                # Update weights\n",
    "                weights *= np.exp(-alpha * y * y_pred)\n",
    "                weights /= np.sum(weights)\n",
    "\n",
    "                # Store the model\n",
    "                self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels using weighted voting.\n",
    "        \"\"\"\n",
    "        model_preds = np.array([model.predict(X) for model in self.models])\n",
    "        weighted_preds = np.zeros(model_preds.shape[1])\n",
    "\n",
    "        for i in range(len(self.models)):\n",
    "            weighted_preds += self.alphas[i] * model_preds[i]\n",
    "\n",
    "        return np.sign(weighted_preds)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "y = np.where(y == 0, -1, 1)  # Convert labels to -1 and 1 for AdaBoost compatibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define diverse base classifiers\n",
    "base_estimators = [\n",
    "    DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "    LogisticRegression(max_iter=500, random_state=42),\n",
    "    SVC(kernel=\"linear\", probability=True, random_state=42),\n",
    "    KNeighborsClassifier(n_neighbors=5),\n",
    "]\n",
    "\n",
    "# Train the AdaBoost ensemble\n",
    "adaboost_model = AdaBoostClassifier(base_estimators=base_estimators, n_estimators=50, random_state=42)\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_adaboost = adaboost_model.predict(X_test)\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred_adaboost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, base_estimators=None, n_estimators=100, max_features='sqrt', random_state=None):\n",
    "        \"\"\"\n",
    "        Random Forest classifier that can use multiple base estimators.\n",
    "        \n",
    "        Parameters:\n",
    "        - base_estimators: List of base models to use for ensemble (e.g., [DecisionTree, LogisticRegression]).\n",
    "        - n_estimators: Total number of models to train.\n",
    "        - max_features: The number of features to use for each model. Options: 'sqrt', 'log2', or an integer.\n",
    "        - random_state: Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.base_estimators = base_estimators or [DecisionTreeClassifier(random_state=random_state)]\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the RandomForest classifier using bootstrap sampling and feature selection.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        self.models = []\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        n_estimators_per_model = self.n_estimators // len(self.base_estimators)\n",
    "\n",
    "        for base_estimator in self.base_estimators:\n",
    "            for _ in range(n_estimators_per_model):\n",
    "                # Bootstrap sampling\n",
    "                indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "                X_bootstrap = X[indices]\n",
    "                y_bootstrap = y[indices]\n",
    "                \n",
    "                # Feature subset selection\n",
    "                if self.max_features == 'sqrt':\n",
    "                    max_features = int(np.sqrt(n_features))\n",
    "                elif self.max_features == 'log2':\n",
    "                    max_features = int(np.log2(n_features))\n",
    "                elif isinstance(self.max_features, int):\n",
    "                    max_features = self.max_features\n",
    "                else:\n",
    "                    max_features = n_features\n",
    "\n",
    "                features = np.random.choice(n_features, size=max_features, replace=False)\n",
    "                X_bootstrap = X_bootstrap[:, features]\n",
    "                \n",
    "                # Train a model on the bootstrap sample with a random subset of features\n",
    "                model = clone(base_estimator)\n",
    "                model.fit(X_bootstrap, y_bootstrap)\n",
    "                self.models.append((model, features))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels using majority voting.\n",
    "        \"\"\"\n",
    "        predictions = np.zeros((len(self.models), len(X)))\n",
    "        for i, (model, features) in enumerate(self.models):\n",
    "            X_subset = X[:, features]\n",
    "            predictions[i, :] = model.predict(X_subset)\n",
    "        \n",
    "        # Majority vote (for classification)\n",
    "        return np.round(np.mean(predictions, axis=0)).astype(int)\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define multiple base classifiers\n",
    "base_estimators = [\n",
    "    DecisionTreeClassifier(max_depth=1, random_state=42),\n",
    "    LogisticRegression(max_iter=500, random_state=42),\n",
    "    KNeighborsClassifier(n_neighbors=5)\n",
    "]\n",
    "\n",
    "# Train the Random Forest ensemble\n",
    "rf_model = RandomForestClassifier(base_estimators=base_estimators, n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Grid Search: {'max_depth': 10, 'min_samples_split': 15, 'n_estimators': 150}\n",
      "Best Parameters from Randomized Search: {'n_estimators': 150, 'min_samples_split': 2, 'max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [10, 20, 30, 50],\n",
    "    'min_samples_split': [2, 5, 10, 15]\n",
    "}\n",
    "\n",
    "# Grid Search for Random Forest\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters from Grid Search:\", grid_search.best_params_)\n",
    "\n",
    "# Randomized Search\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10, 20]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=42), param_distributions=param_dist, n_iter=10, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters from Randomized Search:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Accuracy: 0.8566666666666667\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86       145\n",
      "           1       0.89      0.82      0.86       155\n",
      "\n",
      "    accuracy                           0.86       300\n",
      "   macro avg       0.86      0.86      0.86       300\n",
      "weighted avg       0.86      0.86      0.86       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final model evaluation on the test set\n",
    "best_model = grid_search.best_estimator_  # or random_search.best_estimator_\n",
    "y_pred_final = best_model.predict(X_test)\n",
    "\n",
    "# Final accuracy and classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Final Model Accuracy:\", accuracy_score(y_test, y_pred_final))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_final))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
